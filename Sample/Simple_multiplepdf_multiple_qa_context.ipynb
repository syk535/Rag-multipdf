{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e25a1e4-27b4-4a31-9048-f2b88ebab1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: openai in d:\\programdata\\anaconda3\\lib\\site-packages (1.99.9)\n",
      "Requirement already satisfied: faiss-cpu in d:\\programdata\\anaconda3\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: tiktoken in d:\\programdata\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: pypdf in d:\\programdata\\anaconda3\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (2.4.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-community openai faiss-cpu tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a6009c-92e3-48a4-bbc6-f168c0e902d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import dependencies\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5b0a9-e750-4bd9-9c34-83eaf7681d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key loaded successfully (will not be displayed)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load API Key from .env file & load API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the contents of the .env file into system environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "print(\"✅ API Key loaded successfully (will not be displayed)\")\n",
    "\n",
    "# Windows-specific: avoid MKL/OpenMP conflicts\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0045e43-1da8-4c30-929f-12094c4408b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files will be loaded:\n",
      " - C:/Users/syk_5/main_SS.pdf\n",
      " - C:/Users/syk_5/Resume.pdf\n",
      "Total pages loaded: 34\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select multiple PDFs via system dialog (tkinter)\n",
    "from tkinter import Tk, filedialog\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# open dialog\n",
    "root = Tk(); root.withdraw()\n",
    "pdf_paths = filedialog.askopenfilenames(\n",
    "    title=\"Select PDF files\",\n",
    "    filetypes=[(\"PDF files\", \"*.pdf\")]\n",
    ")\n",
    "root.destroy()\n",
    "\n",
    "pdf_paths = list(pdf_paths)\n",
    "if not pdf_paths:\n",
    "    raise SystemExit(\"No PDF selected. Exiting.\")\n",
    "\n",
    "print(\"The following files will be loaded:\")\n",
    "for p in pdf_paths:\n",
    "    print(\" -\", p)\n",
    "\n",
    "# load all, keep filename+page metadata\n",
    "documents = []\n",
    "for path in pdf_paths:\n",
    "    docs = PyPDFLoader(path).load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(d.metadata.get(\"source\", path))\n",
    "    documents.extend(docs)\n",
    "print(f\"Total pages loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85872b30-81bb-41bc-b6f3-a2e0b4913a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287d2f9d-b990-4967-967d-007ad6ba2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate vector database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babae67d-5e49-4df4-8ee3-ee1210e5d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build a controllable RAG chain with chat memory (LCEL)\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 1) Use your existing vectorstore to create a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})  # tune k later\n",
    "\n",
    "# 2) Chat model is better for multi-turn\n",
    "#llm = ChatOpenAI(temperature=0)  # you can set model=... if needed\n",
    "llm = ChatOpenAI(temperature=0, timeout=60, max_retries=1)\n",
    "\n",
    "# 3) Prompt with a strict system rule and a chat history placeholder\n",
    "#SYSTEM = \"\"\"You must answer ONLY using the provided context.\n",
    "#If the answer is not contained in the context, say \"I don't know.\"\n",
    "#Cite sources like [filename p.X] after claims when possible.\"\"\"\n",
    "SYSTEM = \"\"\"\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "# 4) Format retrieved docs (filename + page for traceability, trimmed for length)\n",
    "def format_docs(docs, max_chars=1200):\n",
    "    rows, seen = [], set()\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        name = Path(meta.get(\"source\", \"doc\")).name\n",
    "        page = meta.get(\"page\")\n",
    "        tag = f\"[{name} p.{(page + 1) if isinstance(page, int) else '?'}]\"\n",
    "        text = d.page_content\n",
    "        key = (name, page, hash(text[:120]))  # light de-dup\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars] + \" ...\"\n",
    "        rows.append(f\"{tag}\\n{text}\")\n",
    "    return \"\\n\\n\".join(rows)\n",
    "\n",
    "# 5) LCEL chain: retrieve -> stitch -> prompt -> LLM -> parse\n",
    "# ✅ FIX: Extract fields instead of passing the whole dict through\n",
    "rag_core = (\n",
    "    # Step 1: Pass through the original question and chat history\n",
    "    RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x.get(\"chat_history\", []),\n",
    "    })\n",
    "    # Step 2: Retrieve relevant documents using the question\n",
    "    | RunnableMap({\n",
    "        \"docs\": lambda x: retriever.invoke(x[\"question\"]),   # Use .invoke to avoid deprecation warnings\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    })\n",
    "    # Step 3: Format retrieved documents into a context string for the LLM\n",
    "    | RunnableMap({\n",
    "        \"context\": lambda x: format_docs(x[\"docs\"]),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"docs\": lambda x: x[\"docs\"],  # Keep docs in the pipeline for later source display\n",
    "    })\n",
    "    # Step 4: In parallel:\n",
    "    #   - Generate the answer using the prompt + LLM + output parser\n",
    "    #   - Pass the docs through unchanged so they are available in the final output\n",
    "    | RunnableMap({\n",
    "        \"answer\": (prompt | llm | StrOutputParser()),\n",
    "        \"docs\":   lambda x: x[\"docs\"],\n",
    "    })\n",
    ")\n",
    "\n",
    "\n",
    "# 6) Add per-session chat history (true multi-turn memory)\n",
    "_store = {}  # in-memory; swap with Redis/SQLite for persistence\n",
    "\n",
    "def _get_history(session_id: str):\n",
    "    if session_id not in _store:\n",
    "        _store[session_id] = ChatMessageHistory()\n",
    "    return _store[session_id]\n",
    "\n",
    "qa = RunnableWithMessageHistory(\n",
    "    rag_core,\n",
    "    get_session_history=_get_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",  # <-- add this line\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9229e468-0db0-43a9-962c-cd7510de5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat mode started. Press Enter on an empty line to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Statistically feasible?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The term \"statistically feasible\" in this context refers to problems for which there exist statistical methods or procedures that can theoretically solve the problem under certain parameter regimes. However, it is important to note that just because a problem is statistically feasible does not necessarily mean that there are computationally efficient procedures available to solve it. In the context provided, examples of statistically feasible but computationally challenging problems include community detection, sparse PCA, and estimation in spiked tensor models. \n",
      "\n",
      "  [source] main_SS.pdf p.8\n",
      "  [source] main_SS.pdf p.28\n",
      "  [source] main_SS.pdf p.28\n",
      "  [source] main_SS.pdf p.5\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Compuationally feasible?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: In the context provided, the term \"computationally feasible\" refers to problems for which there exist efficient algorithms or procedures that can be used to solve them within a reasonable amount of time and computational resources. The text mentions that there are numerous problems that are statistically feasible but lack computationally efficient procedures for solving them. Examples include community detection, sparse PCA, and estimation in spiked tensor models.\n",
      "\n",
      "The text also discusses the transformation of the independence testing problem to the planted clique detection problem, which is known to be computationally hard. By designing runtime-efficient algorithms, the researchers were able to identify structural patterns between graphs and successfully conduct statistical and computational feasible independence tests on data sets such as C.elegans and Wikipedia data. \n",
      "\n",
      "  [source] main_SS.pdf p.8\n",
      "  [source] main_SS.pdf p.26\n",
      "  [source] main_SS.pdf p.2\n",
      "  [source] Resume.pdf p.1\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the trade-off between them?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The trade-off between statistical feasibility and computational feasibility is a common consideration in data analysis and research. \n",
      "\n",
      "Statistical feasibility refers to the ability to theoretically solve a problem using statistical methods or procedures under certain parameter regimes. This involves ensuring that the statistical methods used are appropriate for the data and that the conclusions drawn are valid and reliable.\n",
      "\n",
      "Computational feasibility, on the other hand, refers to the ability to solve a problem efficiently using computational resources. This involves developing algorithms or procedures that can handle large datasets and complex analyses in a reasonable amount of time.\n",
      "\n",
      "The trade-off between statistical feasibility and computational feasibility often arises when dealing with complex data analysis problems. In some cases, statistical methods that are theoretically sound may be computationally intensive and difficult to implement on large datasets. On the other hand, computationally efficient methods may sacrifice some statistical rigor for the sake of speed and scalability.\n",
      "\n",
      "Researchers and data analysts must carefully consider this trade-off when designing experiments, choosing statistical methods, and interpreting results. Striking a balance between statistical and computational feasibility is essential to ensure that the analysis is both accurate and practical. \n",
      "\n",
      "  [source] main_SS.pdf p.20\n",
      "  [source] main_SS.pdf p.27\n",
      "  [source] main_SS.pdf p.2\n",
      "  [source] main_SS.pdf p.23\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What issues might happen for multiple graphs?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: When dealing with multiple graphs, such as three or more graphs, several issues may arise, especially in the context of independence testing for inhomogeneous random graphs. Some of the key issues include:\n",
      "\n",
      "1. Joint Distribution Complexity: When working with multiple graphs, the joint distributions of the graphs cannot always be specified using only the marginal distributions and pairwise edge correlations. This complexity arises because the relationships between the graphs are more intricate and cannot be fully captured by pairwise correlations alone.\n",
      "\n",
      "2. Statistical vs. Computational Trade-off: Analyzing multiple graphs can lead to a trade-off between statistical detectability and computational complexity. In some cases, the correlations between the graphs may be statistically detectable, but the algorithms required to analyze them may have running times that are quasi-polynomial in the number of vertices. Balancing statistical power with computational efficiency becomes a challenge in such scenarios.\n",
      "\n",
      "3. Formulating Appropriate Notions of Correlation: One of the difficulties in independence testing for graphs is formulating an appropriate notion of correlation for graph-valued data. Unlike traditional Euclidean vectors, graphs have a more complex structure that requires specialized methods for measuring correlations and dependencies between them.\n",
      "\n",
      "4. Algorithmic Challenges: Analyzing multiple graphs introduces algorithmic challenges, especially when trying to develop efficient and accurate methods for testing independence between them. The complexity of the relationships and interactions between multiple graphs can make it challenging to design algorithms that are both statistically valid and computationally feasible.\n",
      "\n",
      "Overall, working with multiple graphs in the context of independence testing and other statistical analyses requires careful consideration of these issues to ensure that the methods used are appropriate, accurate, and efficient. \n",
      "\n",
      "  [source] main_SS.pdf p.25\n",
      "  [source] main_SS.pdf p.1\n",
      "  [source] main_SS.pdf p.1\n",
      "  [source] main_SS.pdf p.2\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Why not involved Omnibus?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The use of Omnibus matrix in the context of recovering latent positions from multiple random graphs is a powerful technique that can overcome limitations of existing approaches that only handle the same distribution. The Omnibus matrix approach allows for the integration of information from multiple graphs with different distributions, providing a more comprehensive and robust analysis.\n",
      "\n",
      "However, the specific reason why the Omnibus approach was not involved in the research project described in the resume and related documents is not explicitly stated. Possible reasons could include:\n",
      "\n",
      "1. Scope of the Research: The research project may have focused on specific aspects of inference and pattern recognition on multiple MRI scans, and the use of the Omnibus matrix approach may not have been deemed necessary or relevant to the research objectives.\n",
      "\n",
      "2. Methodological Considerations: The researchers may have chosen alternative methods or algorithms that were more suitable for the specific data and research questions at hand. The Omnibus approach, while powerful, may not always be the most appropriate or efficient method for every research scenario.\n",
      "\n",
      "3. Time and Resource Constraints: Implementing the Omnibus matrix approach and deriving algorithms for recovering latent positions from multiple random graphs can be computationally intensive and time-consuming. The researchers may have opted for more streamlined or readily available methods to meet project deadlines and resource constraints.\n",
      "\n",
      "Overall, while the Omnibus matrix approach is a valuable tool for analyzing multiple graphs with different distributions, its absence in the described research project may have been influenced by various factors related to the research scope, methodology, and practical considerations. \n",
      "\n",
      "  [source] Resume.pdf p.1\n",
      "  [source] main_SS.pdf p.28\n",
      "  [source] main_SS.pdf p.28\n",
      "  [source] Resume.pdf p.1\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Interactive loop (blank line to exit)\n",
    "session_id = \"default_session\"  # change if you need multiple concurrent sessions\n",
    "\n",
    "print(\"Chat mode started. Press Enter on an empty line to exit.\\n\")\n",
    "while True:\n",
    "    query = input(\"You: \").strip()\n",
    "    if query == \"\":\n",
    "        print(\"Bye.\")\n",
    "        break\n",
    "\n",
    "    # Invoke the chain with session-bound memory\n",
    "    res = qa.invoke(\n",
    "        {\"question\": query},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # res is {\"answer\": str, \"docs\": List[Document]}\n",
    "    print(\"Bot:\", res[\"answer\"], \"\\n\")\n",
    "\n",
    "    # (Optional) print sources for transparency\n",
    "    for d in res[\"docs\"]:\n",
    "        meta = d.metadata or {}\n",
    "        name = Path(meta.get(\"source\", \"doc\")).name\n",
    "        page = meta.get(\"page\")\n",
    "        p = (page + 1) if isinstance(page, int) else \"?\"\n",
    "        print(f\"  [source] {name} p.{p}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f9086-734d-45d2-9085-5917ce8b2fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
