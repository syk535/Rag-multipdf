{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e25a1e4-27b4-4a31-9048-f2b88ebab1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: openai in d:\\programdata\\anaconda3\\lib\\site-packages (1.99.9)\n",
      "Requirement already satisfied: faiss-cpu in d:\\programdata\\anaconda3\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: tiktoken in d:\\programdata\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: pypdf in d:\\programdata\\anaconda3\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (2.4.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-community openai faiss-cpu tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a6009c-92e3-48a4-bbc6-f168c0e902d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import dependencies\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5b0a9-e750-4bd9-9c34-83eaf7681d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key loaded successfully (will not be displayed)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load API Key from .env file & load API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the contents of the .env file into system environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "print(\"✅ API Key loaded successfully (will not be displayed)\")\n",
    "\n",
    "# Windows-specific: avoid MKL/OpenMP conflicts\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0045e43-1da8-4c30-929f-12094c4408b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files will be loaded:\n",
      " - C:/Users/syk_5/Resume.pdf\n",
      " - C:/Users/syk_5/main_SS.pdf\n",
      "Total pages loaded: 34\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select multiple PDFs via system dialog (tkinter)\n",
    "from tkinter import Tk, filedialog\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# open dialog\n",
    "root = Tk(); root.withdraw()\n",
    "pdf_paths = filedialog.askopenfilenames(\n",
    "    title=\"Select PDF files\",\n",
    "    filetypes=[(\"PDF files\", \"*.pdf\")]\n",
    ")\n",
    "root.destroy()\n",
    "\n",
    "pdf_paths = list(pdf_paths)\n",
    "if not pdf_paths:\n",
    "    raise SystemExit(\"No PDF selected. Exiting.\")\n",
    "\n",
    "print(\"The following files will be loaded:\")\n",
    "for p in pdf_paths:\n",
    "    print(\" -\", p)\n",
    "\n",
    "# load all, keep filename+page metadata\n",
    "documents = []\n",
    "for path in pdf_paths:\n",
    "    docs = PyPDFLoader(path).load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(d.metadata.get(\"source\", path))\n",
    "    documents.extend(docs)\n",
    "print(f\"Total pages loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70d73a4-10df-4984-b1f6-f5023c309857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid search helpers ---\n",
    "from itertools import product\n",
    "from statistics import mean\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def _faiss_topk_sims(vs: FAISS, query: str, k: int = 4) -> List[float]:\n",
    "    \"\"\"Convert FAISS (doc, distance) to similarity scores.\"\"\"\n",
    "    docs_scores = vs.similarity_search_with_score(query, k=k)\n",
    "    sims = []\n",
    "    for _, dist in docs_scores:\n",
    "        try:\n",
    "            sim = 1.0 / (1.0 + float(dist))\n",
    "        except Exception:\n",
    "            sim = 0.0\n",
    "        sims.append(sim)\n",
    "    return sims\n",
    "\n",
    "def _build_vs_for_params(documents, embeddings, chunk_size: int, chunk_overlap: int) -> Tuple[FAISS, int]:\n",
    "    \"\"\"Split with given params and build a FAISS index. Returns (vs, num_chunks).\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    vs = FAISS.from_documents(chunks, embeddings)\n",
    "    return vs, len(chunks)\n",
    "\n",
    "def grid_search_chunk_params(\n",
    "    queries: List[str],\n",
    "    documents,\n",
    "    embeddings,\n",
    "    chunk_sizes: List[int],\n",
    "    overlaps: List[int],\n",
    "    k: int = 4,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Try (chunk_size, overlap) combos; score via mean top-k similarity across queries.\"\"\"\n",
    "    results = []\n",
    "    best = None\n",
    "    for cs, ov in product(chunk_sizes, overlaps):\n",
    "        vs, n_chunks = _build_vs_for_params(documents, embeddings, cs, ov)\n",
    "        per_q_scores = []\n",
    "        for q in queries:\n",
    "            sims = _faiss_topk_sims(vs, q, k=k)\n",
    "            per_q_scores.append(mean(sims) if sims else 0.0)\n",
    "        avg_score = mean(per_q_scores) if per_q_scores else 0.0\n",
    "        row = {\"chunk_size\": cs, \"overlap\": ov, \"avg_score\": avg_score, \"num_chunks\": n_chunks}\n",
    "        results.append(row)\n",
    "        if (best is None) or (avg_score > best[\"avg_score\"]):\n",
    "            best = row\n",
    "    results_sorted = sorted(results, key=lambda r: r[\"avg_score\"], reverse=True)\n",
    "    return {\"best_params\": best, \"scoreboard\": results_sorted}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85872b30-81bb-41bc-b6f3-a2e0b4913a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search best: {'chunk_size': 400, 'overlap': 200, 'avg_score': 0.7176904205215795, 'num_chunks': 345}\n",
      "Total chunks: 309 (chunk_size=400, overlap=200)\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 3 (Optional): pick chunk_size & overlap via grid search, then split =====\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# init embeddings BEFORE grid search\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# representative queries for evaluation (small set)\n",
    "eval_queries = [\n",
    "    \"What is statistically feasible?\",\n",
    "    \"computationally feasible?\",\n",
    "    \"What is the trade-off between them?\"\n",
    "]\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 1000\n",
    "DEFAULT_OVERLAP = 100\n",
    "use_grid_search = True# False  # set True to enable grid search\n",
    "\n",
    "if use_grid_search:\n",
    "    try:\n",
    "        # (ensure you already defined grid_search_chunk_params somewhere above)\n",
    "        res = grid_search_chunk_params(\n",
    "            queries=eval_queries,\n",
    "            documents=documents,      # Step 2 output\n",
    "            embeddings=embeddings,\n",
    "            chunk_sizes=[400, 700, 1000],\n",
    "            overlaps=[50, 100, 200],\n",
    "            k=4,\n",
    "        )\n",
    "        best_params = res[\"best_params\"] or {}\n",
    "        CHUNK_SIZE = int(best_params.get(\"chunk_size\", DEFAULT_CHUNK_SIZE))\n",
    "        OVERLAP    = int(best_params.get(\"overlap\", DEFAULT_OVERLAP))\n",
    "        print(\"Grid search best:\", best_params)\n",
    "    except Exception as e:\n",
    "        print(\"Grid search failed, fallback to defaults:\", e)\n",
    "        CHUNK_SIZE, OVERLAP = DEFAULT_CHUNK_SIZE, DEFAULT_OVERLAP\n",
    "else:\n",
    "    CHUNK_SIZE, OVERLAP = DEFAULT_CHUNK_SIZE, DEFAULT_OVERLAP\n",
    "\n",
    "# Split with chosen params\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# (Optional) filter overly short chunks that hurt retrieval/refinement\n",
    "docs = [d for d in docs if len(d.page_content) >= 300]\n",
    "\n",
    "print(f\"Total chunks: {len(docs)} (chunk_size={CHUNK_SIZE}, overlap={OVERLAP})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287d2f9d-b990-4967-967d-007ad6ba2e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 4: Build the vector database (reuse the SAME embeddings) =====\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "print(\"FAISS index built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babae67d-5e49-4df4-8ee3-ee1210e5d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helpers for refinement ---\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from copy import deepcopy\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a); nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0: \n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "def _embed_texts(embeddings, texts: List[str]) -> np.ndarray:\n",
    "    vecs = embeddings.embed_documents(texts)\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "def _embed_query(embeddings, query: str) -> np.ndarray:\n",
    "    return np.array(embeddings.embed_query(query), dtype=np.float32)\n",
    "\n",
    "def _generate_candidate_windows(\n",
    "    text: str,\n",
    "    max_shift: int = 300,\n",
    "    step: int = 50,\n",
    "    min_len: int = 300,\n",
    "    max_len: int = 1200,\n",
    "    length_scales=(0.6, 0.8, 1.0)\n",
    ") -> List[str]:\n",
    "    L = len(text)\n",
    "    candidates, seen = [], set()\n",
    "    if L <= min_len:\n",
    "        return [text]\n",
    "    for scale in length_scales:\n",
    "        target = int(max(min_len, min(max_len, L * scale)))\n",
    "        if target <= 0 or target > L:\n",
    "            continue\n",
    "        base = max(0, (L - target) // 2)\n",
    "        starts = {0, max(0, L - target), base}\n",
    "        for shift in range(-max_shift, max_shift + 1, step):\n",
    "            s = max(0, min(base + shift, max(0, L - target)))\n",
    "            starts.add(s)\n",
    "        for s in sorted(starts):\n",
    "            e = min(L, s + target)\n",
    "            seg = text[s:e]\n",
    "            key = (len(seg), hash(seg[:160]))\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                candidates.append(seg)\n",
    "    candidates.append(text if L <= max_len else text[:max_len])\n",
    "    out, seen2 = [], set()\n",
    "    for c in candidates:\n",
    "        k = (len(c), hash(c[:200]))\n",
    "        if k not in seen2:\n",
    "            seen2.add(k)\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def refine_topk_chunks(\n",
    "    query: str,\n",
    "    docs: List[Document],\n",
    "    embeddings,\n",
    "    max_shift: int = 300,\n",
    "    step: int = 50,\n",
    "    min_len: int = 300,\n",
    "    max_len: int = 1200,\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    if not docs:\n",
    "        return [], []\n",
    "    q_vec = _embed_query(embeddings, query)\n",
    "    refined_docs, info = [], []\n",
    "    for d in docs:\n",
    "        base_text = d.page_content or \"\"\n",
    "        cands = _generate_candidate_windows(\n",
    "            base_text, max_shift=max_shift, step=step, min_len=min_len, max_len=max_len\n",
    "        )\n",
    "        cand_vecs = _embed_texts(embeddings, cands)\n",
    "        sims = [_cosine_sim(q_vec, v) for v in cand_vecs]\n",
    "        best_idx = int(np.argmax(sims)) if sims else 0\n",
    "        best_text = cands[best_idx] if sims else base_text\n",
    "        best_score = sims[best_idx] if sims else 0.0\n",
    "        # original (raw or truncated) score\n",
    "        try:\n",
    "            orig_idx = cands.index(base_text if len(base_text) <= max_len else base_text[:max_len])\n",
    "            orig_score = sims[orig_idx]\n",
    "        except ValueError:\n",
    "            orig_score = 0.0\n",
    "        rd = deepcopy(d); rd.page_content = best_text\n",
    "        refined_docs.append(rd)\n",
    "        info.append({\n",
    "            \"source\": (d.metadata or {}).get(\"source\"),\n",
    "            \"page\": (d.metadata or {}).get(\"page\"),\n",
    "            \"orig_len\": len(base_text),\n",
    "            \"refined_len\": len(best_text),\n",
    "            \"orig_score\": orig_score,\n",
    "            \"best_score\": best_score,\n",
    "            \"improvement\": best_score - orig_score,\n",
    "            \"candidates\": len(cands),\n",
    "        })\n",
    "    return refined_docs, info\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 30, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, timeout=60, max_retries=1)\n",
    "SYSTEM = \"\"\"\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs, max_chars=1200):\n",
    "    rows, seen = [], set()\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        name = Path(meta.get(\"source\", \"doc\")).name\n",
    "        page = meta.get(\"page\")\n",
    "        tag = f\"[{name} p.{(page + 1) if isinstance(page, int) else '?'}]\"\n",
    "        text = d.page_content\n",
    "        key = (name, page, hash(text[:120]))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars] + \" ...\"\n",
    "        rows.append(f\"{tag}\\n{text}\")\n",
    "    return \"\\n\\n\".join(rows)\n",
    "\n",
    "use_refinement = True  # set False to disable refinement\n",
    "\n",
    "rag_core = (\n",
    "    RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x.get(\"chat_history\", []),\n",
    "    })\n",
    "    | RunnableMap({\n",
    "        \"docs_raw\":    lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\":    lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    })\n",
    "    # refinement branch (optional)\n",
    "    | (\n",
    "        RunnableLambda(lambda x: (lambda docs, meta: {\n",
    "                \"docs\": docs, \"refine_info\": meta,\n",
    "                \"question\": x[\"question\"], \"chat_history\": x[\"chat_history\"]\n",
    "            })(*refine_topk_chunks(\n",
    "                query=x[\"question\"], docs=x[\"docs_raw\"], embeddings=embeddings,\n",
    "                max_shift=300, step=50, min_len=300, max_len=1200\n",
    "            )))\n",
    "        if use_refinement\n",
    "        else RunnableLambda(lambda x: {\n",
    "            \"docs\": x[\"docs_raw\"], \"refine_info\": [],\n",
    "            \"question\": x[\"question\"], \"chat_history\": x[\"chat_history\"]\n",
    "        })\n",
    "    )\n",
    "    | RunnableMap({\n",
    "        \"context\":      lambda x: format_docs(x[\"docs\"]),\n",
    "        \"question\":     lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"docs\":         lambda x: x[\"docs\"],\n",
    "        \"refine_info\":  lambda x: x[\"refine_info\"],\n",
    "    })\n",
    "    | RunnableMap({\n",
    "        \"answer\":   (prompt | llm | StrOutputParser()),\n",
    "        \"docs\":     lambda x: x[\"docs\"],\n",
    "        \"snippets\": lambda x: [d.page_content for d in x[\"docs\"]],\n",
    "        \"sources\":  lambda x: [\n",
    "            {\n",
    "                \"source\": Path((d.metadata or {}).get(\"source\", \"doc\")).name,\n",
    "                \"page\":   ((d.metadata or {}).get(\"page\") + 1) if isinstance((d.metadata or {}).get(\"page\"), int) else None\n",
    "            } for d in x[\"docs\"]\n",
    "        ],\n",
    "        \"refine_info\": lambda x: x[\"refine_info\"],\n",
    "    })\n",
    ")\n",
    "\n",
    "_store = {}\n",
    "def _get_history(session_id: str):\n",
    "    if session_id not in _store:\n",
    "        _store[session_id] = ChatMessageHistory()\n",
    "    return _store[session_id]\n",
    "\n",
    "qa = RunnableWithMessageHistory(\n",
    "    rag_core,\n",
    "    get_session_history=_get_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9229e468-0db0-43a9-962c-cd7510de5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat mode started. Press Enter on an empty line to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is statistical feasibility?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Statistical feasibility refers to the ability to conduct statistical analysis or tests within certain parameter regimes. It involves determining whether it is possible to obtain meaningful statistical results given the available data and the statistical methods being used. In the context provided, it is mentioned that there are problems that are statistically feasible with certain parameter regimes, but there may not be computationally efficient methods available to analyze them. This highlights the importance of considering both statistical and computational feasibility when conducting data analysis. \n",
      "\n",
      "Sources:\n",
      "  - main_SS.pdf p.8\n",
      "  - Resume.pdf p.1\n",
      "  - main_SS.pdf p.26\n",
      "  - main_SS.pdf p.23\n",
      "  - main_SS.pdf p.25\n",
      "\n",
      "Retrieved snippets:\n",
      "  [1] F →∞ is therefore not sufficient.\n",
      "2.2 Computational Feasibility\n",
      "The results in Section 2.1 provides a necessary condition for statistical detectability. There are, however, numerous\n",
      "problems that are statistically feasible with parameter re ...\n",
      "  [2] improving existing semiparametric two-sample hypothesis tests on random dot product graphs\n",
      "Projects\n",
      "• Predicting Repayment Abilities in Home Credit Loans\n",
      "◦ Identiﬁed critical factors such as payment percentage for credit to enhance loan rep ...\n",
      "  [3] completeness. In Proceedings of the 2017 Annual ACM-SIAM Symposium on Discrete Algorithms , pp. 1326–1341.\n",
      "Chatterjee, S. (2015). Matrix estimation by universal singular value thresholding. Annals of Statistics 43 , 177–214.\n",
      "Chen, L., J. T. ...\n",
      "  [4] an approximate p-value of 5 ×10−4. We thus reject the null hypothesis in favor of the alternative hypothesis that\n",
      "the English and French Wikipedia networks are correlated.\n",
      "We next quantify the degree of correlations between the edges of Ae  ...\n",
      "  [5] interest, especially if this measure also leads to thresholds that are both necessary and sufficient for our independence\n",
      "testing problem.\n",
      "Continuing on the above theme, the critical region for our test statistics in Section 3.1 and Section ...\n",
      "\n",
      "Refinement details (per chunk):\n",
      "  [1] main_SS.pdf p.8 | orig=0.8382 -> best=0.8465 (Δ=0.0083) | len 313 -> 300\n",
      "  [2] Resume.pdf p.1 | orig=0.7857 -> best=0.7874 (Δ=0.0018) | len 392 -> 300\n",
      "  [3] main_SS.pdf p.26 | orig=0.7819 -> best=0.7819 (Δ=0.0000) | len 397 -> 397\n",
      "  [4] main_SS.pdf p.23 | orig=0.7729 -> best=0.7733 (Δ=0.0004) | len 397 -> 317\n",
      "  [5] main_SS.pdf p.25 | orig=0.7997 -> best=0.7997 (Δ=0.0000) | len 372 -> 372\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the difference between computational feasibility?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Computational feasibility refers to the ability to perform computational tasks or algorithms within reasonable time and resource constraints. It involves assessing whether the computational resources available are sufficient to carry out the required computations efficiently. In the context provided, it is mentioned that there are problems that are statistically feasible with certain parameter regimes, but there may not be computationally efficient methods available to analyze them. This highlights the importance of considering both statistical and computational feasibility when conducting data analysis. \n",
      "\n",
      "Sources:\n",
      "  - main_SS.pdf p.8\n",
      "  - main_SS.pdf p.3\n",
      "  - main_SS.pdf p.8\n",
      "  - main_SS.pdf p.15\n",
      "  - main_SS.pdf p.26\n",
      "\n",
      "Retrieved snippets:\n",
      "  [1] F →∞ is therefore not sufficient.\n",
      "2.2 Computational Feasibility\n",
      "The results in Section 2.1 provides a necessary condition for statistical detectability. There are, however, numerous\n",
      "problems that are statistically feasible with parameter re ...\n",
      "  [2] and show that there are significant correlations between the two connectomes; this confirms the observation made in\n",
      "Chen et al. (2016) where the authors showed that, for their vertex nomination tasks, using both connectomes lead to\n",
      "better a ...\n",
      "  [3]  2020). We now present an example of this phenomenon\n",
      "in the context of independence testing. In particular, we show the presence of a statistical vs. computational gap\n",
      "by transforming the independence testing problem to the well-known plant ...\n",
      "  [4] factors involves more tedious book-keeping but is, otherwise, conceptually identical and leads to similar results as\n",
      "those described below.\n",
      "Given (A,B) sampled from the R-ER(P,Q) graphon model we once again let C be the matrix with entries\n",
      " ...\n",
      "  [5] completeness. In Proceedings of the 2017 Annual ACM-SIAM Symposium on Discrete Algorithms , pp. 1326–1341.\n",
      "Chatterjee, S. (2015). Matrix estimation by universal singular value thresholding. Annals of Statistics 43 , 177–214.\n",
      "Chen, L., J. T. ...\n",
      "\n",
      "Refinement details (per chunk):\n",
      "  [1] main_SS.pdf p.8 | orig=0.8188 -> best=0.8270 (Δ=0.0082) | len 313 -> 300\n",
      "  [2] main_SS.pdf p.3 | orig=0.7560 -> best=0.7650 (Δ=0.0091) | len 344 -> 300\n",
      "  [3] main_SS.pdf p.8 | orig=0.8017 -> best=0.8206 (Δ=0.0190) | len 359 -> 300\n",
      "  [4] main_SS.pdf p.15 | orig=0.7542 -> best=0.7542 (Δ=0.0000) | len 370 -> 370\n",
      "  [5] main_SS.pdf p.26 | orig=0.7721 -> best=0.7721 (Δ=0.0000) | len 397 -> 397\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Which one is easier to achieve?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Based on the provided context, it is stated that \"while ∥R∥F →∞ for all of these examples, it is nevertheless easier, both statistically and computationally, to...\" Unfortunately, the sentence is incomplete and does not provide a clear answer to which one, statistical feasibility or computational feasibility, is easier to achieve. \n",
      "\n",
      "However, in general terms, the ease of achieving statistical feasibility versus computational feasibility can vary depending on the specific problem, available resources, and the complexity of the statistical methods or computational algorithms involved. In some cases, statistical feasibility may be easier to achieve if the data is well-structured and the statistical methods are well-established. On the other hand, computational feasibility may be easier if efficient algorithms and computational resources are readily available. \n",
      "\n",
      "Sources:\n",
      "  - main_SS.pdf p.24\n",
      "  - Resume.pdf p.1\n",
      "  - main_SS.pdf p.23\n",
      "  - main_SS.pdf p.2\n",
      "  - main_SS.pdf p.24\n",
      "\n",
      "Retrieved snippets:\n",
      "  [1] procedure that also runs in time polynomial in n. We now mention three directions for future research.\n",
      "Comparing the theoretical results in Theorem 3 and Theorem 4 against either Remark 2 or Theorem 2, we see\n",
      "that while ∥R∥F →∞ for all of t ...\n",
      "  [2] ◦ Selected feature with coordinate descent process ofcategorical PCAwith L1, L2 penalization and trained alogistic regression\n",
      "and XGBoost model achieving 0.8 AUC\n",
      "• YouTube Advertising Strategy for Road Trip App\n",
      "◦ Analyzed YouTube trending d ...\n",
      "  [3] to Algebraic Geometry, and two vertices are connected if there is a hyperlink between the corresponding articles in\n",
      "the English Wikipedia. The second network, denote as Af, consists of 1382 vertices and 29946 edges corresponding to\n",
      "the same ...\n",
      "  [4] Leung and Drton (2018); Sz´ ekely et al. (2007) among others) typically assumes that one have access to iid samples\n",
      "{(Xk,Yk)}n\n",
      "k=1\n",
      "iid\n",
      "∼FXY and wish to determine if FXY = FXFY where FX and FY are the marginal distributions of\n",
      "the {Xk}and {Y ...\n",
      "  [5] people places dates things math things categories\n",
      "people .501 .419 .336 .381 .375 .417\n",
      "places .419 .318 .269 .296 .272 .307\n",
      "dates .336 .269 .278 .227 .148 .159\n",
      "things .381 .296 .227 .267 .238 .282\n",
      "math things .375 .272 .148 .238 .192 .231\n",
      "c ...\n",
      "\n",
      "Refinement details (per chunk):\n",
      "  [1] main_SS.pdf p.24 | orig=0.7680 -> best=0.7680 (Δ=0.0000) | len 324 -> 324\n",
      "  [2] Resume.pdf p.1 | orig=0.7353 -> best=0.7353 (Δ=0.0000) | len 319 -> 319\n",
      "  [3] main_SS.pdf p.23 | orig=0.7356 -> best=0.7356 (Δ=0.0000) | len 347 -> 347\n",
      "  [4] main_SS.pdf p.2 | orig=0.7467 -> best=0.7470 (Δ=0.0002) | len 340 -> 300\n",
      "  [5] main_SS.pdf p.24 | orig=0.7392 -> best=0.7393 (Δ=0.0001) | len 384 -> 300\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye.\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 6: Interactive loop (blank to exit) =====\n",
    "session_id = \"default_session\"\n",
    "print(\"Chat mode started. Press Enter on an empty line to exit.\\n\")\n",
    "while True:\n",
    "    query = input(\"You: \").strip()\n",
    "    if query == \"\":\n",
    "        print(\"Bye.\")\n",
    "        break\n",
    "\n",
    "    res = qa.invoke(\n",
    "        {\"question\": query},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    print(\"Bot:\", res[\"answer\"], \"\\n\")\n",
    "\n",
    "    if \"sources\" in res:\n",
    "        print(\"Sources:\")\n",
    "        for s in res[\"sources\"]:\n",
    "            p = f\" p.{s['page']}\" if s.get(\"page\") else \"\"\n",
    "            print(f\"  - {s['source']}{p}\")\n",
    "        print()\n",
    "\n",
    "    if \"snippets\" in res:\n",
    "        print(\"Retrieved snippets:\")\n",
    "        MAX_PREVIEW = 240\n",
    "        for i, snip in enumerate(res[\"snippets\"], 1):\n",
    "            preview = snip if len(snip) <= MAX_PREVIEW else snip[:MAX_PREVIEW] + \" ...\"\n",
    "            print(f\"  [{i}] {preview}\")\n",
    "        print()\n",
    "\n",
    "    if \"refine_info\" in res and res[\"refine_info\"]:\n",
    "        print(\"Refinement details (per chunk):\")\n",
    "        for i, inf in enumerate(res[\"refine_info\"], 1):\n",
    "            src = inf.get(\"source\")\n",
    "            pg = inf.get(\"page\")\n",
    "            print(\n",
    "                f\"  [{i}] {src} p.{pg+1 if isinstance(pg,int) else '?'} | \"\n",
    "                f\"orig={inf['orig_score']:.4f} -> best={inf['best_score']:.4f} \"\n",
    "                f\"(Δ={inf['improvement']:.4f}) | len {inf['orig_len']} -> {inf['refined_len']}\"\n",
    "            )\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f9086-734d-45d2-9085-5917ce8b2fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
