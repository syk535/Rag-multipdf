{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e25a1e4-27b4-4a31-9048-f2b88ebab1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: openai in d:\\programdata\\anaconda3\\lib\\site-packages (1.99.9)\n",
      "Requirement already satisfied: faiss-cpu in d:\\programdata\\anaconda3\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: tiktoken in d:\\programdata\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: pypdf in d:\\programdata\\anaconda3\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (2.4.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-community openai faiss-cpu tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a6009c-92e3-48a4-bbc6-f168c0e902d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import dependencies\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5b0a9-e750-4bd9-9c34-83eaf7681d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key loaded successfully (will not be displayed)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load API Key from .env file & load API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the contents of the .env file into system environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the key from environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "print(\"✅ API Key loaded successfully (will not be displayed)\")\n",
    "\n",
    "# Windows-specific: avoid MKL/OpenMP conflicts\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0045e43-1da8-4c30-929f-12094c4408b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files will be loaded:\n",
      " - C:/Users/syk_5/Resume.pdf\n",
      " - C:/Users/syk_5/main_SS.pdf\n",
      "Total pages loaded: 34\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select multiple PDFs via system dialog (tkinter)\n",
    "from tkinter import Tk, filedialog\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# open dialog\n",
    "root = Tk(); root.withdraw()\n",
    "pdf_paths = filedialog.askopenfilenames(\n",
    "    title=\"Select PDF files\",\n",
    "    filetypes=[(\"PDF files\", \"*.pdf\")]\n",
    ")\n",
    "root.destroy()\n",
    "\n",
    "pdf_paths = list(pdf_paths)\n",
    "if not pdf_paths:\n",
    "    raise SystemExit(\"No PDF selected. Exiting.\")\n",
    "\n",
    "print(\"The following files will be loaded:\")\n",
    "for p in pdf_paths:\n",
    "    print(\" -\", p)\n",
    "\n",
    "# load all, keep filename+page metadata\n",
    "documents = []\n",
    "for path in pdf_paths:\n",
    "    docs = PyPDFLoader(path).load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(d.metadata.get(\"source\", path))\n",
    "    documents.extend(docs)\n",
    "print(f\"Total pages loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85872b30-81bb-41bc-b6f3-a2e0b4913a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287d2f9d-b990-4967-967d-007ad6ba2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate vector database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "babae67d-5e49-4df4-8ee3-ee1210e5d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Cosine similarity for two numpy vectors\n",
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a); nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0: \n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "def _embed_texts(embeddings, texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Batch-embed texts -> (N, D) numpy array.\"\"\"\n",
    "    vecs = embeddings.embed_documents(texts)  # list[list[float]]\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "def _embed_query(embeddings, query: str) -> np.ndarray:\n",
    "    return np.array(embeddings.embed_query(query), dtype=np.float32)\n",
    "\n",
    "def _generate_candidate_windows(\n",
    "    text: str,\n",
    "    max_shift: int = 200,         # how far to shift left/right (in characters)\n",
    "    step: int = 100,              # shift stride\n",
    "    min_len: int = 300,           # min window length\n",
    "    max_len: int = 1200,          # cap window length\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Create small variations (sub-windows) inside a chunk by trimming a bit from\n",
    "    left/right. This is 'local' refinement without original doc offsets.\n",
    "    \"\"\"\n",
    "    L = len(text)\n",
    "    if L <= min_len:\n",
    "        return [text]  # too short, keep as is\n",
    "\n",
    "    # target length: clamp to [min_len, max_len]\n",
    "    target = max(min(L, max_len), min_len)\n",
    "\n",
    "    # base centered window indices\n",
    "    base_start = max(0, (L - target) // 2)\n",
    "    base_end = min(L, base_start + target)\n",
    "\n",
    "    starts = set()\n",
    "    # try centered + small shifts left/right\n",
    "    for shift in range(-max_shift, max_shift + 1, step):\n",
    "        s = base_start + shift\n",
    "        s = max(0, min(s, max(0, L - target)))\n",
    "        starts.add(s)\n",
    "\n",
    "    # also try hugging left and right edges\n",
    "    starts.add(0)\n",
    "    starts.add(max(0, L - target))\n",
    "\n",
    "    candidates = []\n",
    "    for s in sorted(starts):\n",
    "        e = min(L, s + target)\n",
    "        candidates.append(text[s:e])\n",
    "    # ensure original text as fallback candidate\n",
    "    candidates.append(text if L <= max_len else text[:max_len])\n",
    "    # dedup\n",
    "    seen, out = set(), []\n",
    "    for c in candidates:\n",
    "        key = (len(c), hash(c[:120]))\n",
    "        if key not in seen:\n",
    "            seen.add(key); out.append(c)\n",
    "    return out\n",
    "\n",
    "def _generate_candidate_windows(\n",
    "    text: str,\n",
    "    max_shift: int = 300,         # maximum shift left/right (in characters)\n",
    "    step: int = 50,               # shift stride (smaller = finer search)\n",
    "    min_len: int = 300,           # minimum candidate window length\n",
    "    max_len: int = 1200,          # maximum candidate window length\n",
    "    length_scales=(0.6, 0.8, 1.0) # try scaled-down windows even if L is within [min,max]\n",
    ") -> List[str]:\n",
    "    L = len(text)\n",
    "    candidates, seen = [], set()\n",
    "\n",
    "    # If the chunk is too short, keep it as is (cannot expand without global offsets)\n",
    "    if L <= min_len:\n",
    "        return [text]\n",
    "\n",
    "    for scale in length_scales:\n",
    "        # Compute target length based on scale, clamped to [min_len, max_len]\n",
    "        target = int(max(min_len, min(max_len, L * scale)))\n",
    "        if target <= 0 or target > L:\n",
    "            continue\n",
    "\n",
    "        # Use center window as baseline, then shift left/right\n",
    "        base = max(0, (L - target) // 2)\n",
    "        starts = {0, max(0, L - target), base}\n",
    "        for shift in range(-max_shift, max_shift + 1, step):\n",
    "            s = max(0, min(base + shift, max(0, L - target)))\n",
    "            starts.add(s)\n",
    "\n",
    "        # Generate candidate windows\n",
    "        for s in sorted(starts):\n",
    "            e = min(L, s + target)\n",
    "            seg = text[s:e]\n",
    "            key = (len(seg), hash(seg[:160]))\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                candidates.append(seg)\n",
    "\n",
    "    # Always ensure the original or truncated version is included\n",
    "    candidates.append(text if L <= max_len else text[:max_len])\n",
    "\n",
    "    # Deduplicate by (length, partial hash)\n",
    "    out, seen2 = [], set()\n",
    "    for c in candidates:\n",
    "        k = (len(c), hash(c[:200]))\n",
    "        if k not in seen2:\n",
    "            seen2.add(k)\n",
    "            out.append(c)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def refine_topk_chunks(\n",
    "    query: str,\n",
    "    docs: List[Document],\n",
    "    embeddings,\n",
    "    max_shift: int = 200,\n",
    "    step: int = 100,\n",
    "    min_len: int = 300,\n",
    "    max_len: int = 1200,\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Refine top-k documents by generating candidate sub-windows inside each chunk,\n",
    "    embedding them, and selecting the one with the highest cosine similarity to the query.\n",
    "\n",
    "    Returns:\n",
    "        refined_docs: list of Documents with page_content replaced by the best window\n",
    "        info: metadata list including original vs refined scores and lengths\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return [], []\n",
    "\n",
    "    q_vec = _embed_query(embeddings, query)\n",
    "\n",
    "    refined_docs: List[Document] = []\n",
    "    info: List[Dict[str, Any]] = []\n",
    "\n",
    "    for d in docs:\n",
    "        base_text = d.page_content or \"\"\n",
    "        cands = _generate_candidate_windows(\n",
    "            base_text, max_shift=max_shift, step=step, min_len=min_len, max_len=max_len\n",
    "        )\n",
    "\n",
    "        # embed candidate windows and compute similarity scores\n",
    "        cand_vecs = _embed_texts(embeddings, cands)\n",
    "        sims = [_cosine_sim(q_vec, v) for v in cand_vecs]\n",
    "\n",
    "        # best candidate\n",
    "        best_idx = int(np.argmax(sims)) if sims else 0\n",
    "        best_text = cands[best_idx] if sims else base_text\n",
    "        best_score = sims[best_idx] if sims else 0.0\n",
    "\n",
    "        # original candidate score (raw chunk or truncated if > max_len)\n",
    "        try:\n",
    "            if len(base_text) <= max_len:\n",
    "                orig_idx = cands.index(base_text)\n",
    "            else:\n",
    "                orig_idx = cands.index(base_text[:max_len])\n",
    "            orig_score = sims[orig_idx]\n",
    "        except ValueError:\n",
    "            orig_score = 0.0\n",
    "\n",
    "        # keep refined document\n",
    "        rd = deepcopy(d)\n",
    "        rd.page_content = best_text\n",
    "        refined_docs.append(rd)\n",
    "\n",
    "        # store detailed info\n",
    "        info.append({\n",
    "            \"source\": (d.metadata or {}).get(\"source\"),\n",
    "            \"page\": (d.metadata or {}).get(\"page\"),\n",
    "            \"orig_len\": len(base_text),\n",
    "            \"refined_len\": len(best_text),\n",
    "            \"orig_score\": orig_score,\n",
    "            \"best_score\": best_score,\n",
    "            \"improvement\": best_score - orig_score,\n",
    "            \"candidates\": len(cands),\n",
    "        })\n",
    "\n",
    "    return refined_docs, info\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Build a controllable RAG chain with chat memory (LCEL) — add snippets\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",                      # optional: more diverse retrieval\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 30, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, timeout=60, max_retries=1)\n",
    "\n",
    "#SYSTEM = \"\"\"You must answer ONLY using the provided context.\n",
    "#If the answer is not contained in the context, say \"I don't know.\"\n",
    "#Cite sources like [filename p.X] after claims when possible.\"\"\"\n",
    "SYSTEM = \"\"\"\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs, max_chars=1200):\n",
    "    rows, seen = [], set()\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        name = Path(meta.get(\"source\", \"doc\")).name\n",
    "        page = meta.get(\"page\")\n",
    "        tag = f\"[{name} p.{(page + 1) if isinstance(page, int) else '?'}]\"\n",
    "        text = d.page_content\n",
    "        key = (name, page, hash(text[:120]))  # light de-dup\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars] + \" ...\"\n",
    "        rows.append(f\"{tag}\\n{text}\")\n",
    "    return \"\\n\\n\".join(rows)\n",
    "\n",
    "# Core pipeline:\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# ... (your llm, prompt, format_docs, memory setup as before)\n",
    "\n",
    "rag_core = (\n",
    "    # 1) pass fields\n",
    "    RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x.get(\"chat_history\", []),\n",
    "    })\n",
    "    # 2) retrieve initial top-k docs\n",
    "    | RunnableMap({\n",
    "        \"docs_raw\":    lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\":    lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    })\n",
    "    # 3) refine those top-k docs locally (window shift/trim inside chunk)\n",
    "    | RunnableLambda(lambda x: (lambda refined_docs, meta: {\n",
    "            \"docs\": refined_docs,\n",
    "            \"refine_info\": meta,\n",
    "            \"question\": x[\"question\"],\n",
    "            \"chat_history\": x[\"chat_history\"],\n",
    "        })(*refine_topk_chunks(\n",
    "            query=x[\"question\"],\n",
    "            docs=x[\"docs_raw\"],\n",
    "            embeddings=embeddings,     # reuse your existing OpenAIEmbeddings()\n",
    "            max_shift=200,\n",
    "            step=100,\n",
    "            min_len=300,\n",
    "            max_len=1200\n",
    "        )))\n",
    "    # 4) build LLM context from refined docs\n",
    "    | RunnableMap({\n",
    "        \"context\":      lambda x: format_docs(x[\"docs\"]),\n",
    "        \"question\":     lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"docs\":         lambda x: x[\"docs\"],\n",
    "        \"refine_info\":  lambda x: x[\"refine_info\"],\n",
    "    })\n",
    "    # 5) in parallel: answer + passthrough docs + snippets + sources + refinement meta\n",
    "    | RunnableMap({\n",
    "        \"answer\":   (prompt | llm | StrOutputParser()),\n",
    "        \"docs\":     lambda x: x[\"docs\"],\n",
    "        \"snippets\": lambda x: [d.page_content for d in x[\"docs\"]],\n",
    "        \"sources\":  lambda x: [\n",
    "            {\n",
    "                \"source\": (d.metadata or {}).get(\"source\"),\n",
    "                \"page\": ((d.metadata or {}).get(\"page\") + 1) if isinstance((d.metadata or {}).get(\"page\"), int) else None\n",
    "            } for d in x[\"docs\"]\n",
    "        ],\n",
    "        \"refine_info\": lambda x: x[\"refine_info\"],\n",
    "    })\n",
    ")\n",
    "\n",
    "# memory wrapper\n",
    "_store = {}\n",
    "def _get_history(session_id: str):\n",
    "    if session_id not in _store:\n",
    "        _store[session_id] = ChatMessageHistory()\n",
    "    return _store[session_id]\n",
    "\n",
    "qa = RunnableWithMessageHistory(\n",
    "    rag_core,\n",
    "    get_session_history=_get_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"  # important to silence tracer expecting 'output'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9229e468-0db0-43a9-962c-cd7510de5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat mode started. Press Enter on an empty line to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is statistical feasibility?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Statistical feasibility refers to the ability to perform statistical analysis or inference on a given problem or dataset within certain parameter regimes. It involves determining whether statistical methods can be effectively applied to the data at hand to draw meaningful conclusions. In the context provided, it is mentioned that there are problems that are statistically feasible, meaning that statistical analysis can be conducted, but there may not be computationally efficient procedures available to solve them. \n",
      "\n",
      "Sources:\n",
      "  - main_SS.pdf p.8\n",
      "  - main_SS.pdf p.28\n",
      "  - main_SS.pdf p.23\n",
      "  - Resume.pdf p.1\n",
      "  - main_SS.pdf p.19\n",
      "\n",
      "Refined snippets:\n",
      "  [1] condition ∥R∥F →∞ is therefore not sufficient.\n",
      "2.2 Computational Feasibility\n",
      "The results in Section 2.1 provides a necessary condition for statistical detectability. There are, however, numerous\n",
      "problems that are statistically feasible with ...\n",
      "  [2] Van der Vaart, A. W. (2000). Asymptotic statistics, Volume 3. Cambridge university press.\n",
      "Varshney, L. R., B. L. Chen, E. Paniagua, D. H. Hall, and D. B. Chklovskii (2011). Structural properties of the\n",
      "caenorhabditis elegans neuronal networ ...\n",
      "  [3] an approximate p-value of 5 ×10−4. We thus reject the null hypothesis in favor of the alternative hypothesis that\n",
      "the English and French Wikipedia networks are correlated.\n",
      "We next quantify the degree of correlations between the edges of Ae  ...\n",
      "  [4] ity detection with the accuracy rate from74% to 90% and robustness from 35% to 100%; Theoretically\n",
      "supported the consistency of hypothesis tests and enhanced their power from60% to 100%\n",
      "Independence and Two-Sample Hypothesis Test for Random ...\n",
      "  [5] e\n",
      "power compared to the theoretical (limiting) values. Here Aand B are R-correlated SBM graphs. The first\n",
      "(resp. second) entry in each cell correspond to the empirical estimate (resp. theoretical value) of the type\n",
      "I error when r = 0 and to ...\n",
      "\n",
      "Refinement details (per chunk):\n",
      "  [1] main_SS.pdf p.8 | orig=0.8263 -> best=0.8342 (Δ=0.0078) | len 898 -> 538\n",
      "  [2] main_SS.pdf p.28 | orig=0.7802 -> best=0.7802 (Δ=0.0000) | len 414 -> 414\n",
      "  [3] main_SS.pdf p.23 | orig=0.7679 -> best=0.7679 (Δ=0.0000) | len 986 -> 986\n",
      "  [4] Resume.pdf p.1 | orig=0.7767 -> best=0.7858 (Δ=0.0090) | len 899 -> 539\n",
      "  [5] main_SS.pdf p.19 | orig=0.7687 -> best=0.7729 (Δ=0.0042) | len 486 -> 388\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Comparison with compuational feasibility?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Statistical feasibility and computational feasibility are related concepts but focus on different aspects of a problem:\n",
      "\n",
      "1. Statistical feasibility: Refers to the ability to perform statistical analysis or inference on a given problem or dataset within certain parameter regimes. It involves determining whether statistical methods can be effectively applied to the data at hand to draw meaningful conclusions. In the context provided, it is mentioned that there are problems that are statistically feasible, meaning that statistical analysis can be conducted.\n",
      "\n",
      "2. Computational feasibility: Refers to the ability to solve a problem using computational methods within reasonable time and resources. It involves determining whether there are computationally efficient procedures available to solve a given problem. In the context provided, it is mentioned that there are problems that are statistically feasible but may not have known computationally efficient procedures for solving them.\n",
      "\n",
      "In summary, statistical feasibility focuses on the ability to conduct statistical analysis, while computational feasibility focuses on the ability to solve a problem using computational methods efficiently. \n",
      "\n",
      "Sources:\n",
      "  - main_SS.pdf p.8\n",
      "  - main_SS.pdf p.27\n",
      "  - main_SS.pdf p.20\n",
      "  - Resume.pdf p.1\n",
      "  - main_SS.pdf p.23\n",
      "\n",
      "Refined snippets:\n",
      "  [1] condition ∥R∥F →∞ is therefore not sufficient.\n",
      "2.2 Computational Feasibility\n",
      "The results in Section 2.1 provides a necessary condition for statistical detectability. There are, however, numerous\n",
      "problems that are statistically feasible with ...\n",
      "  [2] Journal of Machine Learning Research 15 , 3513–3540.\n",
      "Lyzinski, V. and D. L. Sussman (2020). Matchability of heterogeneous networks pairs. Information and Inference:\n",
      "A Journal of the IMA 9 , 749–783.\n",
      "27\n",
      "  [3] are correlated. This conclusion, while biologically relevant, is also certainly expected.\n",
      "20\n",
      "  [4] e) between graphs throughout\n",
      "improving existing semiparametric two-sample hypothesis tests on random dot product graphs\n",
      "Projects\n",
      "• Predicting Repayment Abilities in Home Credit Loans\n",
      "◦ Identiﬁed critical factors such as payment percentage f ...\n",
      "  [5] an approximate p-value of 5 ×10−4. We thus reject the null hypothesis in favor of the alternative hypothesis that\n",
      "the English and French Wikipedia networks are correlated.\n",
      "We next quantify the degree of correlations between the edges of Ae  ...\n",
      "\n",
      "Refinement details (per chunk):\n",
      "  [1] main_SS.pdf p.8 | orig=0.8092 -> best=0.8140 (Δ=0.0048) | len 898 -> 538\n",
      "  [2] main_SS.pdf p.27 | orig=0.7768 -> best=0.7768 (Δ=0.0000) | len 201 -> 201\n",
      "  [3] main_SS.pdf p.20 | orig=0.7639 -> best=0.7639 (Δ=0.0000) | len 92 -> 92\n",
      "  [4] Resume.pdf p.1 | orig=0.7725 -> best=0.7835 (Δ=0.0110) | len 924 -> 739\n",
      "  [5] main_SS.pdf p.23 | orig=0.7691 -> best=0.7691 (Δ=0.0000) | len 986 -> 986\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Interactive loop (blank line to exit)\n",
    "session_id = \"default_session\"\n",
    "\n",
    "print(\"Chat mode started. Press Enter on an empty line to exit.\\n\")\n",
    "while True:\n",
    "    query = input(\"You: \").strip()\n",
    "    if query == \"\":\n",
    "        print(\"Bye.\")\n",
    "        break\n",
    "\n",
    "    res = qa.invoke(\n",
    "        {\"question\": query},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    \n",
    "    # After res = qa.invoke(...)\n",
    "\n",
    "    print(\"Bot:\", res[\"answer\"], \"\\n\")\n",
    "    \n",
    "    # Show normalized sources\n",
    "    if \"sources\" in res:\n",
    "        print(\"Sources:\")\n",
    "        for s in res[\"sources\"]:\n",
    "            p = f\" p.{s['page']}\" if s.get(\"page\") else \"\"\n",
    "            print(f\"  - {s['source']}{p}\")\n",
    "        print()\n",
    "    \n",
    "    # Show refined snippets (trimmed)\n",
    "    if \"snippets\" in res:\n",
    "        print(\"Refined snippets:\")\n",
    "        MAX_PREVIEW = 240\n",
    "        for i, snip in enumerate(res[\"snippets\"], 1):\n",
    "            preview = snip if len(snip) <= MAX_PREVIEW else snip[:MAX_PREVIEW] + \" ...\"\n",
    "            print(f\"  [{i}] {preview}\")\n",
    "        print()\n",
    "    \n",
    "    # Show refinement meta (scores, lengths)\n",
    "    if \"refine_info\" in res:\n",
    "        print(\"Refinement details (per chunk):\")\n",
    "        for i, inf in enumerate(res[\"refine_info\"], 1):\n",
    "            src = inf.get(\"source\")\n",
    "            pg = inf.get(\"page\")\n",
    "            print(f\"  [{i}] {src} p.{pg+1 if isinstance(pg,int) else '?'} | \"\n",
    "                  f\"orig={inf['orig_score']:.4f} -> best={inf['best_score']:.4f} \"\n",
    "                  f\"(Δ={inf['improvement']:.4f}) | len {inf['orig_len']} -> {inf['refined_len']}\")\n",
    "\n",
    "        print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f9086-734d-45d2-9085-5917ce8b2fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
